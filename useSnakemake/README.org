#+TITLE: TSCC and Snakemake
#+author: Songpeng Zu
#+date: 2023-05-18
#+OPTIONS: toc:3
#+STARTUP: indent

* Content and Goal
** Content:
In this page, I mainly talk about
- how to submit jobs in TSCC
- especially how to organize jobs using snakemake
Though the pipeline is specific towards TSCC, the principles are
shared for others' HPC(High-Performance Computing) system. And the
idea of describing the depenecies among tasks demonstrate a general
pattern of organizing complex projects.
** Goal:
1. present a global picture about how jobs are scheduled in TSCC
2. provide a relatively complete reference for later reading when using
3. list the resources mentioned in this page so that readers could
  check them in details.
* Introduction to TSCC
** [[https://www.sdsc.edu/services/hpc/tscc/index.html][TSCC]], Triton Shared Computing Cluster in UCSD.
*** Login nodes:
- when you enter tscc using SSH, you are assigned to these nodes
- *SSH Keys*: fast access without password
  - like how we set github SSH Keys
  - See *Generating SSH Keys* in [[https://www.sdsc.edu/support/user_guides/tscc.html][TSCC]]
- Use [[https://www.ssh.com/academy/ssh/config][SSH config]] to simplify ssh commands with a demo
  - You can paste this into the file ~/.ssh/config
  - Next time, you can use =ssh tscc= as a short command.
  - NOTE: replace all the contents within *[]* including *[* and *]*
  #+BEGIN_SRC ssh
    Host *
        ControlMaster auto
        ServerAliveInterval 5
        ServerAliveCountMax 2
        ControlPath ~/.ssh/control-%h-%p-%r
        ControlPersist yes

    Host tscc
        HostName [tscc login node addres]
        User [your name]
        IdentityFile ~/.ssh/[your private key file]
  #+END_SRC
*** Computing nodes:
- when you submit jobs (either interactive or not) with *qsub*
- [[https://www.sdsc.edu/services/hpc/tscc/condo_details.html][TSCC condo program]]
  - CPU Nodes:
    - 2[Not sure]*18 cores, 256GB RAM in toal (7GB/core)
    - 2[Not sure]*32 cores, 1TB RAM in total (15GB/core)
  - GPU Nodes:
    - CPU model: 2[Not sure]*32 cores, 256GB RAM in total (15GB/core)
    - NVIDIA Ampere A100, 40GB GRAM
*** Data-intensive nodes:
- named with *dm1*
- specific ones for data-intensive works, like copy big data.
- No needs to submit jobs, just run your commands.
*** Storage
- *OASIS*: 800+ TB Data Oasis Lustre-based high performance parallel
  file system
  - Each use has *25TB* space.
  - *90-day* purge policy
  - Use =touch [your file]= to update the file timestamp
  - Use =find . -exec touch {} \;= to update the files in current
    directories.
  - Efficiently to handle large files; not good to write many small
    files.
- *HOME*: =~=, 100 GB per user
  - Jobs' output should be written into this.
    - This NFS file system has only one server handling all the
      metadata and storage requirements.
- *PROJECT*: condo storage we buy
  - Typically 300 TB shared by all the users in our lab
  - use =df -h [project dir]= to check the space information
- *TMPDIR*
  - handle temp many files and will be removed after job is done
** [[https://www.sdsc.edu/services/hpc/tscc/hotel_details.html][SU]]: how we pay to TSCC
*** 1 SU: 1 core * 1 hour
- core resources will be calculated *once* we submit jobs
- time resources will be *re-estimated*
- $250 (10K SUs): 1 SU ~ $0.025
*** how to check money left:
- =gbalance -u [user name]=
- add one line ~alias mymoney="gbalance -u [user name]"~ into your
  =~/.bashrc= file and ~source ~/.bashrc~. Then use ~mymoney~ to
  check your status.
** Queue: assign a queue when you submitting a job
- *hotel*
  - max walltime: 168 hours (1 week); max cores/user: 128
- *home*
  - max walltime: unlimited; max cores/user: unlimited
- *glean*: free of charge but may be stoped by system at any time
  - max walltime: 8 hours; max cores/users: 1024
- gpu-related queues:
  - *gpu-hotel*: like *hotel*
  - *gpu-condo*: max walltime: 8 hours; max cores/user: 84
* Submit jobs in TSCC
** Job manager / schedular in HPC
- TORQUE Resource Manager (or Portable Batch System, PBS)
  - TSCC now uses this
- [[https://slurm.schedmd.com/documentation.html][Slurm]] workload manager
  - [[https://www.youtube.com/watch?v=qf3iMO4wer8][TSCC 2.0]] will use this
** Typical PBS script
*** A draft of PBS script
#+BEGIN_SRC bash
#! /bin/bash
#PBS -q glean
#PBS -N test_pbs
#PBS -l nodes=1:ppn=1
#PBS -l walltime=[hh:mm:ss]
#PBS -o [output file]
#PBS -e [error file]
#PBS -V
#PBS -M [email address list]
#PBS -m abe
#PBS -A ren-group
[All the shell commands you want to have here]
#+END_SRC
- Create a script like the one above then =qsub [the_script]=
- Use =qstat -u [user name]= to get the status of the submitted job
*** Interactive job
- =qsub -I -q glean -l nodes=1:ppn=2 -l walltime=08:00:00=
- Add ~alias myjob="qsub -I -q glean -l nodes=1:ppn=2 -l
  walltime=08:00:00"~ to your ~/.bashrc, then =source ~./bashrc=.
  You can then use =myjob= to quickly start an interactive job
  without needing to remember the details.
* [[https://snakemake.github.io][Snakemake]]
** Jobs manager:
- handle jobs' dependencies
- automatically run the pipeline from where it failed
- if some intermedia files / scripts are updated, then automatically
  update all the later rules depend on them.
** Snakemake = Snake[Python] + make[GNU make]
*** [[https://en.wikipedia.org/wiki/Make_(software)][make]]: dependency-tracking build utilities, written in 1976
- Still widely used now, especially [[https://www.gnu.org/software/make/manual/make.html#Introduction][GNU make]]
- Drawback:
  - Lack of configuration file support, like *yaml*, *json*.
  - Lack of support for jobs on HPC
*** Snakemake:
- written in Python, which makes it simple to use
- support config files like *yaml*, *json*.
- support HPC: both PBS and Slurm
*** How it looks like:
#+BEGIN_SRC snakemake
   ## optional config file
   configfile: "config.yaml"
   content = "say hi"
   samples = ["a", "b"]
   ## all the output files you want to have
   rule all:
       input:
          expand("flag/pref_{s}.done", s = samples)
          expand("flag/first_{s}.done", s = samples)

   ## then set up the rules about how to generate them
   rule pre:
       output:
          # s will be infered based on rule all output
          # here it will be a or b.
          # snakemake will run s=a and s=b in parallel if possible
          # touch will be automatically  generate the flag file
          # once the rule is done.
          touch("flag/pre_{s}.done")
       log:
          "log/{s}.log"
       shell:
          """
          # wildcards.s to get the a / b
          echo "pre:" {content} {wildcards.s}
          """
  rule first:
      # snakemake will know that it depends on the output of pre
      # then the rule will run after pre
      input:
          "flag/pre_{s}.done"
      output:
          touch("flag/first_{s}.done")
      shell:
          """
          echo "first:" {content} {wildcards.s}
          """
#+END_SRC
- Then save the above into a file named *demo.snakefile*.
- =snakemake -f demo.snakefile= to run the snakemake.
** Summary
- snakemake uses *input* and *output* of *rules* to organize the depencies of tasks.
- The *input* in *rule all* is used to claim all the final outputs
- *wildcards* will be inferred and the key mechanism to run multiple
  jobs in parallel
* Use Snakemake to control the jobs in TSCC
** Use [[https://github.com/snakemake-profiles/doc][profile]] to setup the particular enviroment of HPC
** Demo for PBS
- =mkdir profile=
- under profile, we create two files.
  - one is *config.yaml*.
    #+BEGIN_SRC yaml
    cluster-config: "profile/cluster.yaml"
    cluster: "qsub -N {cluster.jobname} -l nodes={cluster.nodes}:ppn={cluster.ppn},mem={cluster.mem},walltime={cluster.walltime} -A {cluster.account} -q {cluster.queue} -M {cluster.email} -m {cluster.mailon} -j {cluster.jobout} -e {cluster.logdir} -V "
    jobs: 100
    #+END_SRC
  - one is *cluster.yaml* .
    #+BEGIN_SRC yaml
      __default__:
          jobname: "{rule}.{wildcards}"
          nodes: 1
          ppn: 1
          walltime: "02:00:00"
          account: "ren-group"
          queue: "glean"
          email: "debug.pie@gmail.com"
          mailon: "ae"
          jobout: "oe"
          log: "{rule}.{wildcards}.tscc.log"
      pre:
          ppn: 1
          queue: "glean"
          walltime: 00:10:00
      first:
          ppn: 1
          queue: "glean"
          walltime: 00:10:00
    #+END_SRC
  - Then =snakemake -f demo.snakemake --profile profile= to submit
    jobs into PBS
** Tips
- Use *screen* in login nodes, then start snakemake
** [[https://github.com/beyondpie/CEMBA_wmb_snATAC/tree/main/snakemake.template][An example]]

  

   
