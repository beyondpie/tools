#+TITLE: Use Snakemake to organize the jobs in TSCC
#+author: Songpeng Zu
#+date: 2023-05-18

In this page, I mainly talk about
- how to submit jobs in TSCC
- especially how to organize jobs using snakemake
  
Though the pipeline is specific towards TSCC, the principles are
shared for others' HPC(High-Performance Computing) system. And the
idea of describing the depenecies among tasks demonstrate a general
pattern of organizing complex projects.

The aims for this page:
1. present a global picture about how jobs are scheduled in TSCC
2. provide a relatively complete reference for later reading when using
3. list the resources mentioned in this page so that readers could
  check them in details

* Introduction
** [[https://www.sdsc.edu/services/hpc/tscc/index.html][TSCC]], Triton Shared Computing Cluster in UCSD.
*** Login nodes:
- when you enter tscc using SSH, you are assigned to these nodes
- *SSH Keys*: fast access without password
  - like how we set github SSH Keys
  - See *Generating SSH Keys* in [[https://www.sdsc.edu/support/user_guides/tscc.html][TSCC]]
- Use [[https://www.ssh.com/academy/ssh/config][SSH config]] to simplify ssh commands with a demo
  - You can paste this into the file ~/.ssh/config
  - Next time, you can use =ssh tscc= as a short command.
  - NOTE: replace all the contents within *[]* including *[* and *]*
  #+BEGIN_SRC ssh
    Host *
        ControlMaster auto
        ServerAliveInterval 5
        ServerAliveCountMax 2
        ControlPath ~/.ssh/control-%h-%p-%r
        ControlPersist yes

    Host tscc
        HostName [tscc login node addres]
        User [your name]
        IdentityFile ~/.ssh/[your private key file]
  #+END_SRC

*** Computing nodes:
- when you submit jobs (either interactive or not) with *qsub*
- [[https://www.sdsc.edu/services/hpc/tscc/condo_details.html][TSCC condo program]]
  - CPU Nodes:
    - 2[Not sure]*18 cores, 256GB RAM in toal (7GB/core)
    - 2[Not sure]*32 cores, 1TB RAM in total (15GB/core)
  - GPU Nodes:
    - CPU model: 2[Not sure]*32 cores, 256GB RAM in total (15GB/core)
    - NVIDIA Ampere A100, 40GB GRAM
*** Data-intensive nodes:
- named with *dm1*
- specific ones for data-intensive works, like copy big data.
- No needs to submit jobs, just run your commands.
*** Storage
- OASIS: 800+ TB Data Oasis Lustre-based high performance parallel
  file system
  - Each use has 25TB space.
  - 90-day purge policy
  - Use =touch [your file]= to update the file timestamp
  - Use =find . -exec touch {} \;= to update the files in current
    directories.
- HOME: =~=, 100 GB per user
- PROJECT: condo storage we buy
  - Typically 300 TB shared by all the users in our lab
  - use =df -h [project dir]= to check the space information
** Queue: assign a queue when you submitting a job
- hotel
** [[https://www.sdsc.edu/services/hpc/tscc/hotel_details.html][SU]]: how we pay to TSCC
- 1 SU: 1 core * 1 hour
  - core resources will be calculated once we submit jobs
  - time resources will be re-estimated based on how long we acutally
    use
- $250 (10K SUs): 1 SU ~ $0.025
- =gbalance -u [user name]= to check the money left
  - add one line ~alias mymoney="gbalance -u [user name]"~ into your
    =~/.bashrc= file and ~source ~/.bashrc~. Then use ~mymoney~ to
    check your status.
* Submit jobs in TSCC
** P
* Snakemake
* Use Snakemake to control the jobs in TSCC

   
